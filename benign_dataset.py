import json
import numpy as np
import h5py
import pandas as pd
import os
import math
import sys
import matplotlib.pyplot as plt
from PIL import Image
import random

FIXED_WIDTH = 384
FINAL_SIZE = 384
SEED = 1

CLASSES = ['benign', 'malicious']

def list_files(dir_name, extension = None):
    paths = []
    for root, _, files in os.walk(dir_name):
        for f in files:
            if (extension is not None and extension is not False and extension in f) or (extension is None):
                paths.append(os.path.join(root, f))
            elif extension is False:
                if not '.' in f:
                    paths.append(os.path.join(root, f))
    return paths

def unison_shuffle(X, y, use_seed):
    assert len(X) == len(y)
    if use_seed == True:
        np.random.seed(SEED)
    p = np.random.permutation(len(X))
    return X[p], y[p]

def filter_file_by_height(filepath, width, min_height = 200, max_height = 500):
    size = os.path.getsize(filepath)
    height = math.ceil(size / FIXED_WIDTH)

    if height >= min_height and height <= max_height:
        return True
    else:
        return False


def create_and_save_image(path_to_input, output_folder):
    # load file
    img = np.fromfile(path_to_input, dtype='uint8')

    # resize to (None, FIXED_WIDTH)
    num_pixels = img.shape[0]
    columns = FIXED_WIDTH
    rows = math.ceil(num_pixels / 384)
    total_size = columns * rows
    img.resize(total_size)
    img = img.reshape((rows, columns))
    
    # convert to RGB using a cmap
    cmap = plt.get_cmap('viridis')
    rgba_img = cmap(img)
    img = np.delete(rgba_img, 3, 2)    
    
    # resize to (FINAL_SIZE, FINAL_SIZE)
    img = np.uint8(img * 255)
    pic = Image.fromarray(img)
    pic = pic.resize((FINAL_SIZE, FINAL_SIZE), Image.ANTIALIAS)

    # save image as a file
    original_file_name = path_to_input.split('/')[-1]
    save_path = os.path.join(output_folder, original_file_name)
    pic.save(save_path + '.jpg')

def create_detection_dataset(input_file = None, output_folder = None, train_data = 0.8, use_seed = True):

    file_paths = []

    with open(input_file, 'r') as f:
        file_paths = f.read()
        file_paths = file_paths.split('\n')
    
    
    m = len(file_paths)
    print('Found {} examples in {}'.format(str(m), input_file))

    # train/dev split
    train_samples = int(0.8 * m)

    # save examples as images in a directory hierarchy taken from
    # https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/
    #
    # examples_output_folder
    # ├── train
    # │   ├── benign
    # │   │   └── <some_hash>.jpg
    # │   └── malicious
    # │       └── <some_hash>.jpg
    # └── dev
    #     ├── benign
    #     │   └── <some_hash>.jpg
    #     └── malicious
    #         └── <some_hash>.jpg
    #
    for i, file_path in enumerate(file_paths):
        label = 0
        label_verbose = CLASSES[label]

        if i < train_samples:
            output_path = os.path.join(output_folder, 'train', label_verbose)
        else:
            output_path = os.path.join(output_folder, 'dev', label_verbose)

        os.makedirs(output_path, exist_ok=True)
        create_and_save_image(file_path, output_path)

if __name__ == '__main__':
    input_file = sys.argv[1]
    output_folder = 'dataset'

    create_detection_dataset(input_file, output_folder)

    print('Successfully created dataset at: ', output_folder)

    