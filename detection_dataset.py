import json
import numpy as np
import h5py
import pandas as pd
import os
import math
import matplotlib.pyplot as plt
from PIL import Image

DETECTION_THRESHOLD = 0.5
FIXED_WIDTH = 384
FINAL_SIZE = 384
SEED = 1

def list_files(dir_name, extension = None):
    paths = []
    for root, _, files in os.walk(dir_name):
        for f in files:
            if (extension is not None and extension is not False and extension in f) or (extension is None):
                paths.append(os.path.join(root, f))
            elif extension is False:
                if not '.' in f:
                    paths.append(os.path.join(root, f))
    return paths

def get_file_image(path):
    # load file
    img = np.fromfile(path, dtype='uint8')

    # resize to (None, FIXED_WIDTH)
    num_pixels = img.shape[0]
    columns = FIXED_WIDTH
    rows = math.ceil(num_pixels / 384)

    total_size = columns * rows
    img.resize(total_size)
    img = img.reshape((rows, columns))
    
    # convert to RGB using a cmap
    cmap = plt.get_cmap('viridis')
    rgba_img = cmap(img)
    img = np.delete(rgba_img, 3, 2)    
    
    # resize to (FINAL_SIZE, FINAL_SIZE)
    img = np.uint8(img * 255)
    pic = Image.fromarray(img)
    pic = pic.resize((FINAL_SIZE, FINAL_SIZE), Image.ANTIALIAS)
    img = np.array(pic)

    # normalize pixel values
    img = img.astype(float) / 255.

    return img

def unison_shuffle(X, y, use_seed):
    assert len(X) == len(y)
    if use_seed == True:
        np.random.seed(SEED)
    p = np.random.permutation(len(X))
    return X[p], y[p]

def filter_file_by_height(filepath, width, min_height = 200, max_height = 500):
    size = os.path.getsize(filepath)
    height = math.ceil(size / FIXED_WIDTH)

    if height >= min_height and height <= max_height:
        return True
    else:
        return False


def create_detection_dataset(input_folder = 'results', train_data = 0.8, test_data=False, use_seed=True):
    # create a dict of <hash>:<detected> pairs
    hashes_detection = {}
    result_files = list_files(input_folder, extension='.json')

    print('Found ' + str(len(result_files))+ ' result files. Using them to create a dataset...')

    for f in result_files:
        with open (f, 'r') as result_file:
            sample_hash = f.split('/')[-1].split('.')[0]
            result = json.loads(result_file.read())
            hashes_detection[sample_hash] = (result['detection_percentage'] > DETECTION_THRESHOLD)

    # initialize X, y, m
    hashes_list = [hsh for hsh in list(hashes_detection.keys()) if filter_file_by_height(os.path.join('malware.complete', hsh), FIXED_WIDTH)]
    m = len(hashes_list)
    print(str(m) + ' malware files left after size filtering...')

    X = np.zeros((m, FINAL_SIZE, FINAL_SIZE, 3))
    # y = np.array(list(hashes_detection.values())).reshape((-1, 1))
    y = np.array([hashes_detection[hsh] for hsh in hashes_list]).reshape((-1, 1))

    # set X values to resized images
    for i, hsh in enumerate(hashes_list):
        path_to_malware_sample = os.path.join('malware.complete', hsh)
        img = get_file_image(path_to_malware_sample)
        X[i, :] = img

    # shuffle dataset
    X, y = unison_shuffle(X, y, use_seed)

    # train/dev split
    train_samples = int(0.8 * m)

    X_train = X[:train_samples, :]
    y_train = y[:train_samples, :]

    if test_data == False:
        X_dev = X[train_samples:, :]
        y_dev = y[train_samples:, :]
        dataset = (X_train, y_train, X_dev, y_dev)
    else:
        train_with_dev_samples = train_samples + (1 - train_samples) // 2
        X_dev = X[train_samples:train_with_dev_samples, :]
        y_dev = y[train_samples:train_with_dev_samples, :]

        X_test = X[train_with_dev_samples:, :]
        y_test = y[train_with_dev_samples:, :]

        dataset = (X_train, y_train, X_dev, y_dev, X_test, y_test)

    return dataset

if __name__ == '__main__':
    input_folder = 'labeled-malware'
    output_folder = 'datasets'
    output_file = 'detection_data_filtered_200x500_'
    output_path = output_folder + '/' + output_file + '.h5'

    X_train, y_train, X_dev, y_dev = create_detection_dataset(input_folder=input_folder)    

    total_len = X_train.shape[0] + X_dev.shape[0]

    output_path += str(total_len)

    os.makedirs(output_folder, exist_ok=True)
    
    hf = h5py.File(output_path, 'w')
    hf.create_dataset('X_train', data=X_train)
    hf.create_dataset('y_train', data=y_train)
    hf.create_dataset('X_dev', data=X_dev)
    hf.create_dataset('y_dev', data=y_dev)

    hf.close()

    print('Successfully created dataset at: ', output_path)

    